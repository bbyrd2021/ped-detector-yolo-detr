{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf4f14-539f-4e71-ac84-0d0e25fa15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Phase 2 - Model Comparison Notebook\n",
    "# YOLOv8n vs RT-DETR-L (Ultralytics)\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e4fbf5-5757-4e21-978f-de9e2b602236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1️⃣ Environment & Paths\n",
    "# ============================================\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"datasets\", \"caltech\", \"images\", \"test\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"reports\", \"phase2_results\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\nDataset: {DATASET_DIR}\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d5195-a7af-40c0-851f-101fda5ba45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2️⃣ Load Models\n",
    "# ============================================\n",
    "print(\"\\nLoading models...\")\n",
    "yolo_model = YOLO(\"yolov8n.pt\")\n",
    "rtdetr_model = YOLO(\"rtdetr-l.pt\")\n",
    "print(\"✅ Models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c8fd2-3d96-45ff-85b6-462656676758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3️⃣ Define Benchmark Function\n",
    "# ============================================\n",
    "def benchmark_dataset(model, name, source_dir, save_dir, max_images=None):\n",
    "    \"\"\"Run inference on all images in a folder and return average FPS.\"\"\"\n",
    "    image_files = [\n",
    "        os.path.join(source_dir, f)\n",
    "        for f in os.listdir(source_dir)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    if max_images:\n",
    "        image_files = image_files[:max_images]\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    total_time = 0\n",
    "    print(f\"\\nRunning {name} on {len(image_files)} images...\")\n",
    "\n",
    "    for img_path in tqdm(image_files):\n",
    "        start = time.time()\n",
    "        results = model(img_path, verbose=False)\n",
    "        total_time += time.time() - start\n",
    "\n",
    "        # Save annotated image\n",
    "        img_name = os.path.basename(img_path)\n",
    "        save_path = os.path.join(save_dir, img_name)\n",
    "        results[0].save(filename=save_path)\n",
    "\n",
    "    avg_time = total_time / len(image_files)\n",
    "    fps = 1.0 / avg_time\n",
    "    print(f\"{name} | Avg Time: {avg_time:.3f}s | FPS: {fps:.2f}\")\n",
    "    return avg_time, fps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
